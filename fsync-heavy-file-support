ext4: Add support for fsync-heavy files

There are some applications and libraries, such as SQLite which do a
large number of frequent, file-appending writes followed by fsync
commands.  I'm aware of one such application which would try to do
over 200 append/fsync operations per second (if barriers were disabled)!

Very often with such workloads, they are either write-mostly (i.e.,
application journal files) or have a random-read pattern, or both.
Hence, it makes sense for ext4 to use a different allocation strategy
for such files.

If this flag is set, mballoc will try very hard to keep the block
allocation close to the inode table, even if this means making the
files more fragmented.  

We should ideally turn this feature on with a hueristic, but for now,
make it be something which needs to be turned on via a file flag.

Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
---
 fs/ext4/balloc.c  |    2 +
 fs/ext4/ext4.h    |   14 +++++++++--
 fs/ext4/extents.c |    9 ++++---
 fs/ext4/inode.c   |    4 ++-
 fs/ext4/mballoc.c |   64 +++++++++++++++++++++++++++++++++++++++++++++++++---
 5 files changed, 81 insertions(+), 12 deletions(-)

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index adf96b8..8c2e768 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -584,6 +584,8 @@ ext4_fsblk_t ext4_new_meta_blocks(handle_t *handle, struct inode *inode,
 	ar.inode = inode;
 	ar.goal = goal;
 	ar.len = count ? *count : 1;
+	if (ext4_test_inode_flag(inode, EXT4_INODE_HEAVY_FSYNC))
+		ar.flags = EXT4_MB_FSYNC_HEAVY_ALLOC;
 
 	ret = ext4_mb_new_blocks(handle, &ar, errp);
 	if (count)
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index bab2387..8d96f88 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -108,6 +108,8 @@ typedef unsigned int ext4_group_t;
 #define EXT4_MB_DELALLOC_RESERVED	0x0400
 /* We are doing stream allocation */
 #define EXT4_MB_STREAM_ALLOC		0x0800
+/* Allocating for an fsync-heavy workload */
+#define EXT4_MB_FSYNC_HEAVY_ALLOC	0x1000
 
 
 struct ext4_allocation_request {
@@ -347,17 +349,21 @@ struct flex_groups {
 #define EXT4_EXTENTS_FL			0x00080000 /* Inode uses extents */
 #define EXT4_EA_INODE_FL	        0x00200000 /* Inode used for large EA */
 #define EXT4_EOFBLOCKS_FL		0x00400000 /* Blocks allocated beyond EOF */
+#define EXT4_HEAVY_FSYNC_FL		0x00800000 /* Inode is frequently 
+						      fsynced with allocating
+						      writes */
 #define EXT4_RESERVED_FL		0x80000000 /* reserved for ext4 lib */
 
-#define EXT4_FL_USER_VISIBLE		0x004BDFFF /* User visible flags */
-#define EXT4_FL_USER_MODIFIABLE		0x004B80FF /* User modifiable flags */
+#define EXT4_FL_USER_VISIBLE		0x00CBDFFF /* User visible flags */
+#define EXT4_FL_USER_MODIFIABLE		0x00CB80FF /* User modifiable flags */
 
 /* Flags that should be inherited by new inodes from their parent. */
 #define EXT4_FL_INHERITED (EXT4_SECRM_FL | EXT4_UNRM_FL | EXT4_COMPR_FL |\
 			   EXT4_SYNC_FL | EXT4_IMMUTABLE_FL | EXT4_APPEND_FL |\
 			   EXT4_NODUMP_FL | EXT4_NOATIME_FL |\
 			   EXT4_NOCOMPR_FL | EXT4_JOURNAL_DATA_FL |\
-			   EXT4_NOTAIL_FL | EXT4_DIRSYNC_FL)
+			   EXT4_NOTAIL_FL | EXT4_DIRSYNC_FL |\
+			   EXT4_HEAVY_FSYNC_FL)
 
 /* Flags that are appropriate for regular files (all but dir-specific ones). */
 #define EXT4_REG_FLMASK (~(EXT4_DIRSYNC_FL | EXT4_TOPDIR_FL))
@@ -404,6 +410,7 @@ enum {
 	EXT4_INODE_EXTENTS	= 19,	/* Inode uses extents */
 	EXT4_INODE_EA_INODE	= 21,	/* Inode used for large EA */
 	EXT4_INODE_EOFBLOCKS	= 22,	/* Blocks allocated beyond EOF */
+	EXT4_INODE_HEAVY_FSYNC	= 23,	/* Inode frequently fsync'ed */
 	EXT4_INODE_RESERVED	= 31,	/* reserved for ext4 lib */
 };
 
@@ -450,6 +457,7 @@ static inline void ext4_check_flag_values(void)
 	CHECK_FLAG_VALUE(EXTENTS);
 	CHECK_FLAG_VALUE(EA_INODE);
 	CHECK_FLAG_VALUE(EOFBLOCKS);
+	CHECK_FLAG_VALUE(HEAVY_FSYNC);
 	CHECK_FLAG_VALUE(RESERVED);
 }
 
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index e910720..0571b06 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -3438,11 +3438,12 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 	ar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);
 	ar.logical = map->m_lblk;
 	ar.len = allocated;
+	ar.flags = 0;
+	if (ext4_test_inode_flag(inode, EXT4_INODE_HEAVY_FSYNC))
+		ar.flags = EXT4_MB_FSYNC_HEAVY_ALLOC;
 	if (S_ISREG(inode->i_mode))
-		ar.flags = EXT4_MB_HINT_DATA;
-	else
-		/* disable in-core preallocation for non-regular files */
-		ar.flags = 0;
+		ar.flags |= EXT4_MB_HINT_DATA;
+
 	newblock = ext4_mb_new_blocks(handle, &ar, &err);
 	if (!newblock)
 		goto out2;
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index e80fc51..35ced86 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -681,9 +681,11 @@ static int ext4_alloc_blocks(handle_t *handle, struct inode *inode,
 	ar.goal = goal;
 	ar.len = target;
 	ar.logical = iblock;
+	if (ext4_test_inode_flag(inode, EXT4_INODE_HEAVY_FSYNC))
+		ar.flags = EXT4_MB_FSYNC_HEAVY_ALLOC;
 	if (S_ISREG(inode->i_mode))
 		/* enable in-core preallocation only for regular files */
-		ar.flags = EXT4_MB_HINT_DATA;
+		ar.flags |= EXT4_MB_HINT_DATA;
 
 	current_block = ext4_mb_new_blocks(handle, &ar, err);
 	if (unlikely(current_block + ar.len > EXT4_MAX_BLOCK_FILE_PHYS)) {
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 851f49b..3cdf069 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2006,7 +2006,7 @@ static noinline_for_stack int
 ext4_mb_regular_allocator(struct ext4_allocation_context *ac)
 {
 	ext4_group_t ngroups, group, i;
-	int cr;
+	int cr, flex_size;
 	int err = 0;
 	struct ext4_sb_info *sbi;
 	struct super_block *sb;
@@ -2015,6 +2015,7 @@ ext4_mb_regular_allocator(struct ext4_allocation_context *ac)
 	sb = ac->ac_sb;
 	sbi = EXT4_SB(sb);
 	ngroups = ext4_get_groups_count(sb);
+	flex_size = ext4_flex_bg_size(sbi);
 	/* non-extent files are limited to low blocks/groups */
 	if (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS)))
 		ngroups = sbi->s_blockfile_groups;
@@ -2058,12 +2059,34 @@ ext4_mb_regular_allocator(struct ext4_allocation_context *ac)
 		spin_unlock(&sbi->s_md_lock);
 	}
 
-	/* Let's just scan groups to find more-less suitable blocks */
-	cr = ac->ac_2order ? 0 : 1;
-	/*
+	/* 
+	 * Let's just scan groups to find more-less suitable blocks
+	 *
 	 * cr == 0 try to get exact allocation,
 	 * cr == 3  try to get anything
+	 *
+	 * If we are doing an fsync-heavy allocation we always want to
+	 * start at the goal group and move forwards, and accept
+	 * anything we can get.  We first scan the first flex_bg
+	 * groups looking for a free extent big enough to hold the
+	 * requested write size.  But if we can't find it there, we'll
+	 * start again looking for any free blocks to keep the
+	 * allocation close to the inode table, even if they won't
+	 * fulfill the requested allocation.  This based on the
+	 * assumption that short-distance seeks are faster than very
+	 * long-distance weeks, so that a few short-distance seeks is
+	 * better than a very long-distance seek.  We may want to this
+	 * tunable, but for now, default this limit to the flex_bg
+	 * size as an arbitrary default.  (For a 4k block size, and a
+	 * default flex_bg size of 16 means we try very hard to keep
+	 * the block within 2GB of the inode table, even if that means
+	 * fragmenting the write.)
 	 */
+	if (ac->ac_flags & EXT4_MB_FSYNC_HEAVY_ALLOC)
+		cr = 2;
+	else
+		cr = ac->ac_2order ? 0 : 1;
+
 repeat:
 	for (; cr < 4 && ac->ac_status == AC_STATUS_CONTINUE; cr++) {
 		ac->ac_criteria = cr;
@@ -2111,6 +2134,10 @@ repeat:
 
 			if (ac->ac_status != AC_STATUS_CONTINUE)
 				break;
+
+			if ((cr == 2) && (i > flex_size) &&
+			    (ac->ac_flags & EXT4_MB_FSYNC_HEAVY_ALLOC))
+				break;
 		}
 	}
 
@@ -2898,12 +2925,41 @@ ext4_mb_normalize_request(struct ext4_allocation_context *ac,
 				struct ext4_allocation_request *ar)
 {
 	int bsbits, max;
+	int flex_size = ext4_flex_bg_size(EXT4_SB(ac->ac_sb));
 	ext4_lblk_t end;
 	loff_t size, orig_size, start_off;
 	ext4_lblk_t start;
 	struct ext4_inode_info *ei = EXT4_I(ac->ac_inode);
 	struct ext4_prealloc_space *pa;
 
+	/*
+	 * hint for a fsync-heavy workload
+	 */
+	if (ac->ac_flags & EXT4_MB_FSYNC_HEAVY_ALLOC) {
+		int block_group = ei->i_block_group;
+		struct ext4_super_block *es = EXT4_SB(ac->ac_sb)->s_es;
+		ext4_group_t group;
+		ext4_grpblk_t block;
+
+		if (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {
+			block_group &= ~(flex_size-1);
+			if (S_ISREG(ac->ac_inode->i_mode) &&
+			    (ac->ac_flags & EXT4_MB_HINT_DATA))
+				block_group++;
+		}
+		ar->goal = ext4_group_first_block_no(ac->ac_sb, block_group);
+		if (ar->goal < le32_to_cpu(es->s_first_data_block) ||
+		    ar->goal >= ext4_blocks_count(es))
+			ar->goal = le32_to_cpu(es->s_first_data_block);
+		ext4_get_group_no_and_offset(ac->ac_sb, ar->goal, &group,
+					     &block);
+		ac->ac_g_ex.fe_group = ac->ac_o_ex.fe_group = group;
+		ac->ac_o_ex.fe_start = ac->ac_o_ex.fe_start = block;
+		ac->ac_flags |= EXT4_MB_HINT_NOPREALLOC;
+		ac->ac_flags &= ~EXT4_MB_HINT_DATA;
+		return;
+	}
+
 	/* do normalize only data requests, metadata requests
 	   do not need preallocation */
 	if (!(ac->ac_flags & EXT4_MB_HINT_DATA))
